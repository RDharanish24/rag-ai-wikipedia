Wikipedia RAG (Local LLM + Streamlit)

A simple Retrieval-Augmented Generation (RAG) application built with:

ğŸ§  LlamaIndex

ğŸ¤— HuggingFace Embeddings

ğŸ”¥ Qwen 2.5 (0.5B Instruct) Local LLM

ğŸ“š Wikipedia as Knowledge Source

ğŸŒ Streamlit UI

This app allows you to ask questions about AI / Machine Learning topics, and it answers using Wikipedia content with a local LLM.

ğŸš€ Features

âœ… Fully Local LLM (No OpenAI API required)

âœ… Wikipedia document retrieval

âœ… HuggingFace embeddings (all-MiniLM-L6-v2)

âœ… Persistent vector index storage

âœ… Streamlit web interface

âœ… CPU compatible (slower but works)

ğŸ“š Topics Included

The RAG index is built from the following Wikipedia pages:

Artificial intelligence

Machine learning

Deep learning

Convolutional neural network

Long short-term memory

ğŸ› ï¸ Tech Stack
Component	Tool Used
Framework	Streamlit
RAG Engine	LlamaIndex
Embeddings	sentence-transformers/all-MiniLM-L6-v2
LLM	Qwen/Qwen2.5-0.5B-Instruct
Data Source	Wikipedia
ğŸ“¦ Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/wiki-rag.git
cd wiki-rag

2ï¸âƒ£ Create Virtual Environment (Recommended)
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Requirements
pip install -r requirements.txt


If you don't have a requirements.txt, install manually:

pip install streamlit torch llama-index llama-index-embeddings-huggingface \
llama-index-llms-huggingface llama-index-readers-wikipedia \
sentence-transformers transformers

â–¶ï¸ Run the App
streamlit run app.py


Then open:

http://localhost:8501

ğŸ“ Project Structure
wiki-rag/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ wiki_rag/          # Persisted vector index
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

âš™ï¸ How It Works

Wikipedia pages are loaded using WikipediaReader

Documents are chunked (512 tokens)

Text embeddings are created using all-MiniLM-L6-v2

Vector index is stored locally (./wiki_rag)

When a question is asked:

Top similar chunk is retrieved

Qwen LLM generates answer from context

Retrieved context is displayed in UI

ğŸ§  Model Details
Embedding Model

sentence-transformers/all-MiniLM-L6-v2

Lightweight and fast

Good for CPU usage

LLM

Qwen/Qwen2.5-0.5B-Instruct

Decoder-only model

2048 context window

Runs locally with torch

ğŸ–¥ï¸ Hardware Requirements

Minimum:

8GB RAM

CPU support

Recommended:

16GB RAM

GPU (for faster generation)

âš ï¸ Notes

First run will take time (downloads model + builds index)

CPU inference may be slow

Index is stored locally in ./wiki_rag

Delete the folder if you want to rebuild the index

ğŸ§© Future Improvements

Add PDF upload support

Increase similarity_top_k

Add chat history memory

Add streaming responses

Add multi-document support